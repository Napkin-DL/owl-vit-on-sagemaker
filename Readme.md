# OWL-ViT on SageMaker

## ì†Œê°œ

OWL-ViT (Vision Transformer for Open-World Localization)ëŠ” Matthias Minderer, Alexey Gritsenko ë“±ì´ "[Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230)" ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ í˜ì‹ ì ì¸ ëª¨ë¸ìž…ë‹ˆë‹¤.

### OWL-ViTëž€?

OWL-ViTëŠ” ë‹¤ì–‘í•œ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒìœ¼ë¡œ í•™ìŠµëœ ê°œë°©í˜• ì–´íœ˜ ê°ì²´ ê°ì§€ ë„¤íŠ¸ì›Œí¬ìž…ë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- ðŸ“ **í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ê¸°ë°˜ ê°ì²´ ê²€ì¶œ**: í•˜ë‚˜ ë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë‚´ì—ì„œ í•´ë‹¹ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…ëœ ëŒ€ìƒ ê°ì²´ë¥¼ ê²€ìƒ‰í•˜ê³  ê°ì§€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
- ðŸŒ **ê°œë°©í˜• ì–´íœ˜**: ì‚¬ì „ ì •ì˜ëœ ê°ì²´ í´ëž˜ìŠ¤ì— ì œí•œë˜ì§€ ì•Šê³ , ë‹¤ì–‘í•œ ê°ì²´ë¥¼ ì¸ì‹í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
- ðŸ–¼ï¸ **ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ í•™ìŠµ**: ë‹¤ì–‘í•œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ìŒì„ í†µí•´ í•™ìŠµë˜ì–´, í’ë¶€í•œ ì‹œê°ì  ì´í•´ì™€ ì–¸ì–´ì  ì—°ê´€ì„±ì„ ê°–ì¶”ê³  ìžˆìŠµë‹ˆë‹¤.

> **ì£¼ëª©í•  ì **: OWL-ViTëŠ” **85M(patch 32 : 492 MB) ~ 87M(patch 16 : 611 MB) íŒŒë¼ë¯¸í„°**ë¡œ êµ¬ì„±ëœ ë¹„êµì  ìž‘ì€ í¬ê¸°ì˜ ëª¨ë¸ìž…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì»´íŒ©íŠ¸í•œ êµ¬ì¡° ë•ë¶„ì— **CPUì—ì„œë„ íš¨ìœ¨ì ì¸ ì¶”ë¡ ì´ ê°€ëŠ¥**í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ íŠ¹ì„±ì€ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤:

- ðŸš€ **ë¹ ë¥¸ ì¶”ë¡  ì†ë„**: ìž‘ì€ ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•´ ì¶”ë¡  ì‹œê°„ì´ ë‹¨ì¶•ë©ë‹ˆë‹¤.
- ðŸ’° **ë¹„ìš© íš¨ìœ¨ì„±**: CPU ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆì–´ GPU ëŒ€ë¹„ ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
- ðŸ”§ **ìœ ì—°í•œ ë°°í¬**: ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì‰½ê²Œ ë°°í¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
- ðŸŽ¯ **ë‹¤ëª©ì  ì‚¬ìš©**: ê°ì²´ ê²€ì¶œ, ì´ë¯¸ì§€ ê²€ìƒ‰, ì‹œê°ì  ì§ˆì˜ì‘ë‹µ ë“± ë‹¤ì–‘í•œ ìž‘ì—…ì— í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì´ repositoryëŠ” ì´ëŸ¬í•œ ê°•ë ¥í•œ OWL-ViT ëª¨ë¸ì„ Amazon SageMakerì—ì„œ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

## ì£¼ìš” ë…¸íŠ¸ë¶ ì†Œê°œ

ì´ repositoryì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë„¤ ê°€ì§€ ì£¼ìš” ì¶”ë¡  ë°©ì‹ì„ ë‹¤ë£¹ë‹ˆë‹¤:

1. **SageMaker Processing Job** [1.SageMaker-Processing-Job.ipynb](1.SageMaker-Processing-Job.ipynb)
2. **SageMaker BatchTransform Job** [2.SageMaker-BatchTransform-Job.ipynb](2.SageMaker-BatchTransform-Job.ipynb)
3. **SageMaker Real-time Inference** [3.SageMaker-Realtime-Inference.ipynb](3.SageMaker-Realtime-Inference.ipynb) 
4. **SageMaker Serverless Inference** [4.SageMaker-Serverless-Inference.ipynb](4.SageMaker-Serverless-Inference.ipynb) 

ê° ë°©ì‹ì— ëŒ€í•œ ìžì„¸í•œ ì„¤ëª…ì€ ì•„ëž˜ì™€ ê°™ìŠµë‹ˆë‹¤.

### 1. SageMaker Processing Job

```python
current_time = strftime("%m%d-%H%M%s")
i_type = instance_type.replace('.','-')
job_name = f'owl-vit-{i_type}-{instance_count}-{current_time}'

eval_processor = FrameworkProcessor(
    PyTorch,
    framework_version="2.3",
    py_version="py311",
    role=role, 
    instance_count=instance_count,
    instance_type=instance_type,
    sagemaker_session=sagemaker_session
    )

eval_processor.run(
    code="evaluation.py",
    source_dir=source_dir,
    wait=False,
    inputs=[ProcessingInput(source=input_image_path, 
                            input_name="test_data", 
                            destination="/opt/ml/processing/data", 
                            s3_data_distribution_type=s3_data_distribution_type),
            ProcessingInput(source=model_weight_path, 
                            input_name="model_weight", 
                            destination="/opt/ml/processing/weights")
    ],
    outputs=[
        ProcessingOutput(source="/opt/ml/processing/output", destination=output_path),
    ],
    arguments=["--threshold", "0.1"],
    job_name=job_name
)
```

- ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
- ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ S3ì— ì €ìž¥
- ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ì í•©

### 2. SageMaker BatchTransform Job

```python
create_model_response = sm_client.create_model(
    ModelName=sm_model_name, 
    ExecutionRoleArn=role, 
    PrimaryContainer=container,
)

import json
texts = json.dumps([["a photo of a tv", "a photo of a dog"]])

env = {"threshold" : "0.1",
       "texts" : texts}

response = sm_client.create_transform_job(
    TransformJobName=job_name,
    ModelName=sm_model_name,
    MaxConcurrentTransforms=2,
    MaxPayloadInMB=2,
    BatchStrategy="SingleRecord", ##'MultiRecord',
    Environment=env,
    TransformInput={
        'DataSource': {
            'S3DataSource': {
                'S3DataType': 'S3Prefix',
                'S3Uri': input_image_path 
            }
        },
        'ContentType': "application/x-image",
    },
    TransformOutput={
        'S3OutputPath': output_path,
        'Accept': 'application/json',
    },
    TransformResources={
        'InstanceType': 'ml.m5.2xlarge',
        'InstanceCount': 1
    }
)
```

- ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- ê° ìž…ë ¥ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ê°œë³„ .out íŒŒì¼ë¡œ S3ì— ì €ìž¥
- ê°œë³„ ê²°ê³¼ ì¶”ì ì´ í•„ìš”í•œ ê²½ìš°ì— ìœ ìš©

### 3. SageMaker Real-time Inference

```python
create_model_response = sm_client.create_model(
    ModelName=sm_model_name, 
    ExecutionRoleArn=role, 
    PrimaryContainer=container,
)

create_endpoint_config_response = sm_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        {
            "InstanceType": instance_type,
            "InitialVariantWeight": 1,
            "InitialInstanceCount": 1,
            "ModelName": sm_model_name,
            "VariantName": "AllTraffic",
        }
    ],
)
create_endpoint_response = sm_client.create_endpoint(
    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name
)
```

- ì‹¤ì‹œê°„ ì¶”ë¡ ì„ ìœ„í•œ ì§€ì†ì ì¸ ì—”ë“œí¬ì¸íŠ¸ ì œê³µ
- ë‚®ì€ ì§€ì—° ì‹œê°„ì´ ìš”êµ¬ë˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©
- 24/7 ê°€ìš©ì„± í•„ìš” ì‹œ ì‚¬ìš©

### 4. SageMaker Serverless Inference

```python
create_model_response = sm_client.create_model(
    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container
)

create_endpoint_config_response = sm_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        {
            "ModelName": sm_model_name,
            "VariantName": "AllTraffic",
            "ServerlessConfig": {
                "MemorySizeInMB": 2048,
                "MaxConcurrency": 2,
                "ProvisionedConcurrency": 1,
            }
        }
    ],
)
create_endpoint_response = sm_client.create_endpoint(
    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name
)
```

- ìš”ì²­ ì‹œì—ë§Œ ì„œë²„ê°€ ë™ìž‘í•˜ëŠ” ì„œë²„ë¦¬ìŠ¤ ë°©ì‹
- ë¹„ìš© íš¨ìœ¨ì ì´ë©° ìžë™ ìŠ¤ì¼€ì¼ë§ ì§€ì›
- ê°„í—ì ì¸ íŠ¸ëž˜í”½ íŒ¨í„´ì— ì í•©
- ì²« í˜¸ì¶œ ì‹œ Cold Start ë°œìƒ ê°€ëŠ¥

## ì„¤ì¹˜ ë° ì‚¬ìš©ë²•

### ì‚¬ìš©ë²•

1. ì´ ì €ìž¥ì†Œë¥¼ í´ë¡ í•©ë‹ˆë‹¤:
```bash
git clone https://github.com/your-username/OWL-ViT-on-SageMaker.git
cd OWL-ViT-on-SageMaker
```

2. ê° ë…¸íŠ¸ë¶ íŒŒì¼ì„ ì—´ì–´ ë‹¨ê³„ë³„ ê°€ì´ë“œë¥¼ ë”°ë¼ ì‹¤í–‰í•˜ì„¸ìš”.


## ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” Apache-2.0 ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.


# OWL-ViT on SageMaker

## Introduction

OWL-ViT (Vision Transformer for Open-World Localization) is an innovative model proposed by Matthias Minderer, Alexey Gritsenko, and others in their paper "[Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230)".

### What is OWL-ViT?

OWL-ViT is an open-vocabulary object detection network trained on various (image, text) pairs. The key features of this model include:

- ðŸ“ **Text Query-Based Object Detection**: It can search and detect target objects described in text within an image using one or multiple text queries.
- ðŸŒ **Open Vocabulary**: It can recognize a wide range of objects without being limited to predefined object classes.
- ðŸ–¼ï¸ **Image-Text Pair Learning**: Trained on diverse image and text pairs, it possesses rich visual understanding and linguistic relevance.

> **Note**: OWL-ViT is a relatively small model with **85M (patch 32: 492 MB) ~ 87M (patch 16: 611 MB) parameters**. This compact structure enables **efficient inference even on CPUs**.

These characteristics offer the following advantages:

- ðŸš€ **Fast Inference Speed**: The small model size reduces inference time.
- ðŸ’° **Cost-Effectiveness**: CPU instances can be used, reducing costs compared to GPUs.
- ðŸ”§ **Flexible Deployment**: Easy to deploy in various environments.
- ðŸŽ¯ **Multipurpose Use**: Applicable to various tasks such as object detection, image search, and visual question answering.

This repository provides methods for inferencing this powerful OWL-ViT model on Amazon SageMaker in various ways.

## Key Notebooks Introduction

This repository covers four main inference methods:

1. **SageMaker Processing Job** [1.SageMaker-Processing-Job.ipynb](1.SageMaker-Processing-Job.ipynb)
2. **SageMaker BatchTransform Job** [2.SageMaker-BatchTransform-Job.ipynb](2.SageMaker-BatchTransform-Job.ipynb)
3. **SageMaker Real-time Inference** [3.SageMaker-Realtime-Inference.ipynb](3.SageMaker-Realtime-Inference.ipynb) 
4. **SageMaker Serverless Inference** [4.SageMaker-Serverless-Inference.ipynb](4.SageMaker-Serverless-Inference.ipynb) 

Detailed explanations for each method are as follows:

### 1. SageMaker Processing Job

```python
current_time = strftime("%m%d-%H%M%s")
i_type = instance_type.replace('.','-')
job_name = f'owl-vit-{i_type}-{instance_count}-{current_time}'

eval_processor = FrameworkProcessor(
    PyTorch,
    framework_version="2.3",
    py_version="py311",
    role=role, 
    instance_count=instance_count,
    instance_type=instance_type,
    sagemaker_session=sagemaker_session
    )

eval_processor.run(
    code="evaluation.py",
    source_dir=source_dir,
    wait=False,
    inputs=[ProcessingInput(source=input_image_path, 
                            input_name="test_data", 
                            destination="/opt/ml/processing/data", 
                            s3_data_distribution_type=s3_data_distribution_type),
            ProcessingInput(source=model_weight_path, 
                            input_name="model_weight", 
                            destination="/opt/ml/processing/weights")
    ],
    outputs=[
        ProcessingOutput(source="/opt/ml/processing/output", destination=output_path),
    ],
    arguments=["--threshold", "0.1"],
    job_name=job_name
)
```

- Processes large amounts of data in batch mode
- Saves results as CSV files in S3
- Suitable for large-scale datasets

### 2. SageMaker BatchTransform Job

```python
create_model_response = sm_client.create_model(
    ModelName=sm_model_name, 
    ExecutionRoleArn=role, 
    PrimaryContainer=container,
)

import json
texts = json.dumps([["a photo of a tv", "a photo of a dog"]])

env = {"threshold" : "0.1",
       "texts" : texts}

response = sm_client.create_transform_job(
    TransformJobName=job_name,
    ModelName=sm_model_name,
    MaxConcurrentTransforms=2,
    MaxPayloadInMB=2,
    BatchStrategy="SingleRecord", ##'MultiRecord',
    Environment=env,
    TransformInput={
        'DataSource': {
            'S3DataSource': {
                'S3DataType': 'S3Prefix',
                'S3Uri': input_image_path 
            }
        },
        'ContentType': "application/x-image",
    },
    TransformOutput={
        'S3OutputPath': output_path,
        'Accept': 'application/json',
    },
    TransformResources={
        'InstanceType': 'ml.m5.2xlarge',
        'InstanceCount': 1
    }
)
```

- Processes large amounts of data in batch mode
- Saves results as individual .out files in S3 for each input
- Useful when individual result tracking is needed

### 3. SageMaker Real-time Inference

```python
create_model_response = sm_client.create_model(
    ModelName=sm_model_name, 
    ExecutionRoleArn=role, 
    PrimaryContainer=container,
)

create_endpoint_config_response = sm_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        {
            "InstanceType": instance_type,
            "InitialVariantWeight": 1,
            "InitialInstanceCount": 1,
            "ModelName": sm_model_name,
            "VariantName": "AllTraffic",
        }
    ],
)
create_endpoint_response = sm_client.create_endpoint(
    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name
)
```

- Provides continuous endpoints for real-time inference
- Suitable for applications requiring low latency
- Used when 24/7 availability is needed

### 4. SageMaker Serverless Inference

```python
create_model_response = sm_client.create_model(
    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container
)

create_endpoint_config_response = sm_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        {
            "ModelName": sm_model_name,
            "VariantName": "AllTraffic",
            "ServerlessConfig": {
                "MemorySizeInMB": 2048,
                "MaxConcurrency": 2,
                "ProvisionedConcurrency": 1,
            }
        }
    ],
)
create_endpoint_response = sm_client.create_endpoint(
    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name
)
```

- Serverless approach where the server operates only upon request
- Cost-effective and supports automatic scaling
- Suitable for intermittent traffic patterns
- Cold start may occur on the first call

## Installation and Usage

### Usage

1. Clone this repository:
```bash
git clone https://github.com/your-username/OWL-ViT-on-SageMaker.git
cd OWL-ViT-on-SageMaker
```

2. Open each notebook file and follow the step-by-step guide.

## License

This project is distributed under the Apache-2.0 license.