{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f30f03e-c48a-47cc-8102-6ca49a74a002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True\n",
    "install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0129cf-804d-4476-8dbf-30fbafdd4660",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already revised\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    sudo service docker stop\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo rsync -aP /var/lib/docker /home/ec2-user/SageMaker/.container\n",
    "    sudo service docker start\n",
    "    echo \"Docker Restart\"\n",
    "fi\n",
    "\n",
    "# sudo curl -L \"https://github.com/docker/compose/releases/download/v2.7.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
    "# sudo chmod +x /usr/local/bin/docker-compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa57513-d4fe-401a-b7c9-69922db6c9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install --upgrade pip --quiet\n",
    "    !{sys.executable} -m pip install -U sagemaker --quiet\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370522f7-db75-4f8f-8b1c-6c7a24afed58",
   "metadata": {},
   "source": [
    "![image](./imgs/processing-job-image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe18e0fb-d4a3-4ed3-a9e5-7a42786e0c95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import huggingface_hub\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch\n",
    "\n",
    "\n",
    "source_dir = f\"{Path.cwd()}/src\"\n",
    "os.makedirs(source_dir, exist_ok=True)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"240929-owl-vit\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c290195e-6f5e-40e8-8eb9-8226b77592aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['HF_DATASETS_CACHE'] = '/home/ec2-user/SageMaker/.cache'\n",
    "os.environ['HF_CACHE_HOME'] = '/home/ec2-user/SageMaker/.cache'\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/home/ec2-user/SageMaker/.cache'\n",
    "# os.environ['TRANSFORMERS_HOME'] = '/home/ec2-user/SageMaker/.cache'\n",
    "# os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1296f331-7b59-43c3-9e19-5340974acf2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_model_id = 'google/owlvit-base-patch32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f3fef9-ab65-4d6f-9045-dad0e8b9580a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd711bc51ea4883a6063da9e8dfd4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53c6d76f-8657-49aa-a6ab-f663abc3f49c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registered_model : owlvit-base-patch32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b440875a668c4e2583f50438959e19b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/2024/inference-code/owl-vit-on-sagemaker/owlvit-base-patch32'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_model = test_model_id.split(\"/\")[-1].lower().replace(\".\", \"-\")\n",
    "print(f\"registered_model : {registered_model}\")\n",
    "os.makedirs(registered_model, exist_ok=True)\n",
    "\n",
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=test_model_id,\n",
    "    revision=\"main\",\n",
    "    local_dir=registered_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcf7c62-bfdd-4755-b2e5-1635cea1e405",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'owlvit-base-patch32'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model_weight = test_model_id.split(\"/\")[-1].lower().replace(\".\", \"-\")\n",
    "local_model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c01521-7a5e-4057-9d33-37eadbaf4604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight spec (in this case, just an S3 path): s3://sagemaker-us-west-2-322537213286/240929-owl-vit/owlvit-base-patch32\n"
     ]
    }
   ],
   "source": [
    "s3_model_weight_path = sagemaker_session.upload_data(path=f'./{local_model_weight}', bucket=bucket, key_prefix=f\"{prefix}/{local_model_weight}\")\n",
    "print('Model weight spec (in this case, just an S3 path): {}'.format(s3_model_weight_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e841f6-f69b-4ad7-ac6a-763167805d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-322537213286/240929-owl-vit/ecommerce-products\n"
     ]
    }
   ],
   "source": [
    "s3_input_data_path = sagemaker_session.upload_data(path=f'./ecommerce-products', bucket=bucket, key_prefix=f\"{prefix}/ecommerce-products\")\n",
    "print('input spec (in this case, just an S3 path): {}'.format(s3_input_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba55141-586a-482c-a4de-b2c6b7f4fd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "len(glob.glob(\"./ecommerce-products/tv/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25453a0b-9cba-4704-8600-4f2e4551f959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/requirements.txt\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbed9ce0-2378-44ef-beb9-ef8cfc337d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/evaluation.py\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "from time import strftime\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "current_time = strftime(\"%m%d-%H%M%s\")\n",
    "hostname = os.environ.get('HOSTNAME').split(\".\")[0]\n",
    "\n",
    "weights_path = \"/opt/ml/processing/weights\"\n",
    "input_path = \"/opt/ml/processing/data\"\n",
    "output_path = Path(f\"/opt/ml/processing/output/{current_time}-test_result-{hostname}.csv\")\n",
    "\n",
    "\n",
    "# processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "# model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "processor = OwlViTProcessor.from_pretrained(weights_path)\n",
    "model = OwlViTForObjectDetection.from_pretrained(weights_path)\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "res = []\n",
    "for image_path in glob.glob(f\"{input_path}/*\"):\n",
    "    print(f\"image_path : {image_path}\")\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")  # RGB로 변환\n",
    "    \n",
    "    texts = [[\"a photo of a tv\", \"a photo of a dog\"]]\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "    target_sizes = torch.Tensor([image.size[::-1]])\n",
    "    # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "    results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\n",
    "    i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "    text = texts[i]\n",
    "    boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "        \n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        confidence = round(score.item(), 3)\n",
    "        label_name = texts[0][label]\n",
    "        print(f\"Detected {label_name} in {image_path} with confidence {confidence} at location {box}\")\n",
    "        res.append([str(image_path), label_name, confidence, box])\n",
    "\n",
    "print(f\"num of results : {len(res)}\")\n",
    "\n",
    "fields = ['image_name', 'label_name', 'confidence', 'location']\n",
    "with output_path.open('w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fields)\n",
    "    writer.writerows(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8576de5b-9022-47e3-a0b6-8cd63a4ef12d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile src/evaluation.py\n",
    "# import os\n",
    "# import csv\n",
    "# import numpy as np\n",
    "# import glob\n",
    "# import argparse\n",
    "# from pathlib import Path\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "# from datetime import datetime\n",
    "# from time import strftime\n",
    "# import multiprocessing as mp\n",
    "\n",
    "# def process_image(image_path, texts, threshold, weights_path):\n",
    "#     processor = OwlViTProcessor.from_pretrained(weights_path)\n",
    "#     model = OwlViTForObjectDetection.from_pretrained(weights_path)\n",
    "    \n",
    "#     print(f\"image_path : {image_path}\")\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image_size = image.size\n",
    "#     image = np.array(image)\n",
    "    \n",
    "#     if image.ndim == 2:\n",
    "#         image = np.stack((image,)*3, axis=-1)\n",
    "\n",
    "#     inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "#     target_sizes = torch.Tensor([image_size[::-1]])\n",
    "#     results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=threshold)[0]\n",
    "\n",
    "#     boxes, scores, labels = results[\"boxes\"], results[\"scores\"], results[\"labels\"]\n",
    "#     detections = []\n",
    "#     for box, score, label in zip(boxes, scores, labels):\n",
    "#         label_name = texts[0][label]\n",
    "#         confidence = round(score.item(), 3)\n",
    "#         box_coords = [round(i, 2) for i in box.tolist()]\n",
    "#         print(f\"Detected {label_name} in {image_path} with confidence {confidence} at location {box_coords}\")\n",
    "#         detections.append((str(image_path), label_name, confidence, box_coords)) \n",
    "    \n",
    "#     return detections\n",
    "\n",
    "# current_time = strftime(\"%m%d-%H%M%s\")\n",
    "# hostname = os.environ.get('HOSTNAME').split(\".\")[0]\n",
    "\n",
    "# weights_path = \"/opt/ml/processing/weights\"\n",
    "# input_path = \"/opt/ml/processing/data\"\n",
    "# output_path = Path(f\"/opt/ml/processing/output/{current_time}-test_result-{hostname}.csv\")\n",
    "\n",
    "# start_time = datetime.now()\n",
    "# print(f\"Job started at: {start_time}\")\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--threshold\", type=float, default=0.1, help=\"confidence threshold\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# texts = [[\"a photo of a tv\", \"a photo of a dog\"]]\n",
    "\n",
    "# image_paths = glob.glob(f\"{input_path}/*\")\n",
    "# print(f\"num of image_paths : {len(image_paths)}\")\n",
    "\n",
    "# with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "#     results = pool.starmap(process_image, [(path, texts, args.threshold, weights_path) for path in image_paths])\n",
    "\n",
    "# # Flatten the results list\n",
    "# res = [item for sublist in results if sublist for item in sublist]\n",
    "# print(f\"num of results : {len(res)}\")\n",
    "# end_time = datetime.now()\n",
    "# total_time = end_time - start_time\n",
    "\n",
    "# fields = ['image_name', 'label_name', 'confidence', 'location']\n",
    "\n",
    "# # Check if file exists to determine whether to write headers\n",
    "# file_exists = output_path.exists()\n",
    "\n",
    "# with output_path.open('w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     if not file_exists:\n",
    "#         writer.writerow(fields)\n",
    "#     writer.writerows(res)\n",
    "\n",
    "# print(f\"Job started at: {start_time}\")\n",
    "# print(f\"Job ended at: {end_time}\")\n",
    "# print(f\"Total execution time: {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "015a9385-eb84-4f18-8820-2282c319376d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.m5.2xlarge\"\n",
    "# instance_type = \"local\"\n",
    "instance_count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60784584-568a-4eb8-bae3-593b417feafa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://sagemaker-us-west-2-322537213286/240929-owl-vit/ecommerce-products/tv',\n",
       " 's3://sagemaker-us-west-2-322537213286/240929-owl-vit/owlvit-base-patch32',\n",
       " 's3://sagemaker-us-west-2-322537213286/240929-owl-vit/output')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if instance_type =='local':\n",
    "    import os\n",
    "    from sagemaker.local import LocalSession\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    input_image_path = f\"{Path.cwd()}/ecommerce-products/tv\"\n",
    "    model_weight_path = f\"{Path.cwd()}/{local_model_weight}\"\n",
    "    output_path = f\"{Path.cwd()}/output\"\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    input_image_path = f\"{s3_input_data_path}/tv\"\n",
    "    model_weight_path = s3_model_weight_path\n",
    "    output_path = f\"s3://{bucket}/{prefix}/output\"\n",
    "    s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "input_image_path, model_weight_path, output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57894605-a6ca-4113-a096-cb0970332e95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-west-2-322537213286/240929-owl-vit/output/1001-03581727755138-test_result-ip-10-0-243-217.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm $output_path --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7ed5ed9-fa57-473f-b6f6-e26b96a0b4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.processing import Processor, ScriptProcessor, FrameworkProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d83cd96-dc27-45b2-b3e9-4c892296f02d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name owl-vit-ml-m5-2xlarge-1-1001-04321727757173\n"
     ]
    }
   ],
   "source": [
    "current_time = strftime(\"%m%d-%H%M%s\")\n",
    "i_type = instance_type.replace('.','-')\n",
    "job_name = f'owl-vit-{i_type}-{instance_count}-{current_time}'\n",
    "\n",
    "eval_processor = FrameworkProcessor(\n",
    "    PyTorch,\n",
    "    framework_version=\"2.3\",\n",
    "    py_version=\"py311\",\n",
    "    role=role, \n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "\n",
    "eval_processor.run(\n",
    "    code=\"evaluation.py\",\n",
    "    source_dir=source_dir,\n",
    "    wait=False,\n",
    "    inputs=[ProcessingInput(source=input_image_path, \n",
    "                            input_name=\"test_data\", \n",
    "                            destination=\"/opt/ml/processing/data\", \n",
    "                            s3_data_distribution_type=s3_data_distribution_type),\n",
    "            ProcessingInput(source=model_weight_path, \n",
    "                            input_name=\"model_weight\", \n",
    "                            destination=\"/opt/ml/processing/weights\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(source=\"/opt/ml/processing/output\", destination=output_path),\n",
    "    ],\n",
    "    arguments=[\"--threshold\", \"0.1\"],\n",
    "    job_name=job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "583aace5-c24f-467f-a156-cbce5a158194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............\u001b[34mCodeArtifact repository not specified. Skipping login.\u001b[0m\n",
      "\u001b[34mWARNING: Skipping typing as it is not installed.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.23.2 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 1)) (2.32.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 1)) (0.4.3)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.21,>=0.20 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading tokenizers-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 1)) (2024.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 1)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2024.8.30)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 33.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 792.8/792.8 kB 42.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 67.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 0.23.0\n",
      "    Uninstalling huggingface_hub-0.23.0:\n",
      "      Successfully uninstalled huggingface_hub-0.23.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed huggingface-hub-0.25.1 regex-2024.9.11 tokenizers-0.20.0 transformers-4.45.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/88.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/11.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/11.jpg with confidence 0.574 at location [5.36, 102.36, 487.99, 385.75]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/129.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/129.jpg with confidence 0.615 at location [2.47, 186.15, 1242.83, 1064.09]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/158.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/158.jpg with confidence 0.194 at location [1035.31, 1513.63, 1406.04, 2240.57]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/158.jpg with confidence 0.13 at location [-32.83, -22.85, 2375.5, 2479.28]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/116.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/116.jpg with confidence 0.159 at location [0.74, 81.23, 299.47, 222.11]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/152.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/152.jpg with confidence 0.408 at location [84.33, 533.45, 2463.1, 1987.2]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/55.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/63.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/63.jpg with confidence 0.168 at location [120.47, 124.63, 2479.86, 1498.4]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/110.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/110.jpg with confidence 0.354 at location [123.21, 113.43, 1095.53, 660.21]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/15.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/14.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/14.jpg with confidence 0.137 at location [18.09, -4.86, 1511.74, 991.96]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/62.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/5.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/5.jpg with confidence 0.397 at location [29.09, 29.97, 964.09, 480.84]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/5.jpg with confidence 0.13 at location [47.68, 503.28, 417.03, 779.15]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/42.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/115.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/115.jpg with confidence 0.51 at location [58.24, 363.17, 1582.79, 1252.01]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/58.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/58.jpg with confidence 0.461 at location [2.16, 87.97, 499.48, 397.29]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/114.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/114.jpg with confidence 0.43 at location [71.23, 333.76, 1457.08, 1191.44]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/113.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/113.jpg with confidence 0.528 at location [50.02, 341.17, 1543.5, 1230.9]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/41.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/41.jpg with confidence 0.263 at location [59.14, 347.57, 1533.67, 1251.82]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/41.jpg with confidence 0.125 at location [54.61, 363.85, 1550.45, 1341.9]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/67.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/142.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/142.jpg with confidence 0.483 at location [98.83, 122.91, 2480.66, 1544.4]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/198.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/198.jpg with confidence 0.556 at location [94.76, 151.3, 404.75, 346.96]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/96.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/100.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/4.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/4.jpg with confidence 0.137 at location [190.76, 351.93, 1035.75, 889.91]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/194.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/194.jpg with confidence 0.268 at location [93.82, 118.82, 2446.0, 1498.6]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/134.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/134.jpg with confidence 0.22 at location [59.4, 277.11, 1484.36, 1375.83]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/50.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/53.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/172.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/172.jpg with confidence 0.341 at location [23.25, 355.15, 477.96, 628.33]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/172.jpg with confidence 0.373 at location [17.97, 358.35, 481.88, 634.66]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/71.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/107.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/107.jpg with confidence 0.259 at location [14.12, 8.18, 1505.61, 897.58]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/9.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/9.jpg with confidence 0.447 at location [65.72, 40.65, 2430.13, 2094.19]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/84.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/84.jpg with confidence 0.258 at location [17.02, 10.59, 1509.16, 895.44]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/33.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/33.jpg with confidence 0.401 at location [110.43, 101.04, 886.38, 946.76]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/119.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/119.jpg with confidence 0.41 at location [19.76, 197.14, 988.91, 816.58]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/61.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/61.jpg with confidence 0.562 at location [101.67, 12.02, 947.19, 809.19]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/31.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/31.jpg with confidence 0.191 at location [29.79, 102.0, 959.14, 912.4]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/22.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/23.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/74.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/74.jpg with confidence 0.258 at location [17.02, 10.59, 1509.16, 895.44]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/167.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/167.jpg with confidence 0.292 at location [9.34, 104.99, 1000.63, 683.06]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/167.jpg with confidence 0.153 at location [1.54, 55.78, 998.41, 984.89]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/192.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/192.jpg with confidence 0.192 at location [0.79, -2.61, 992.68, 571.46]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/65.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/65.jpg with confidence 0.392 at location [46.49, 80.79, 1547.59, 1439.22]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/196.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/196.jpg with confidence 0.635 at location [12.91, 355.0, 489.6, 639.63]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/196.jpg with confidence 0.122 at location [19.54, 360.72, 482.3, 629.16]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/28.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/28.jpg with confidence 0.113 at location [90.3, 132.16, 2358.85, 1487.3]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/28.jpg with confidence 0.156 at location [45.18, 50.79, 2403.72, 2488.81]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/28.jpg with confidence 0.208 at location [1050.73, 1529.16, 1424.31, 2267.75]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/138.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/138.jpg with confidence 0.291 at location [96.19, 340.51, 1908.02, 1467.95]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/19.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/29.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/29.jpg with confidence 0.192 at location [30.32, 104.0, 960.8, 913.7]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/34.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/34.jpg with confidence 0.556 at location [38.94, 235.41, 982.17, 822.94]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/32.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/32.jpg with confidence 0.569 at location [61.62, 196.53, 953.77, 808.84]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/120.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/79.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/191.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/93.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/7.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/130.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/130.jpg with confidence 0.133 at location [109.03, 383.61, 1479.03, 1210.32]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/106.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/106.jpg with confidence 0.318 at location [448.39, 323.89, 1541.11, 968.84]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/149.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/51.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/159.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/159.jpg with confidence 0.523 at location [5.76, 88.75, 990.47, 688.03]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/159.jpg with confidence 0.113 at location [433.26, 741.16, 567.31, 913.57]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/112.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/112.jpg with confidence 0.458 at location [48.11, 141.39, 957.78, 831.55]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/80.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a dog in /opt/ml/processing/data/80.jpg with confidence 0.108 at location [774.68, 53.89, 1100.01, 554.97]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/46.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/46.jpg with confidence 0.371 at location [54.52, 226.74, 1483.86, 1495.43]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/184.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/184.jpg with confidence 0.412 at location [89.11, 579.47, 2484.97, 2042.52]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/164.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/164.jpg with confidence 0.512 at location [54.11, 225.53, 934.67, 780.64]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/199.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/199.jpg with confidence 0.159 at location [1.01, 138.81, 991.76, 723.69]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/199.jpg with confidence 0.166 at location [418.87, 715.18, 567.0, 909.27]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/186.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/186.jpg with confidence 0.142 at location [198.83, 170.61, 2465.23, 1464.31]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/91.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/91.jpg with confidence 0.542 at location [25.1, 211.09, 955.67, 797.99]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/123.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/123.jpg with confidence 0.3 at location [146.35, 101.61, 1336.8, 779.55]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/89.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/155.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/155.jpg with confidence 0.36 at location [69.08, 396.98, 1527.08, 1245.99]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/157.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/157.jpg with confidence 0.36 at location [69.08, 396.98, 1527.08, 1245.99]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/57.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/102.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/102.jpg with confidence 0.648 at location [50.47, 254.94, 958.64, 838.39]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/90.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/78.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/78.jpg with confidence 0.124 at location [107.81, 7.3, 1010.18, 595.34]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/151.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/151.jpg with confidence 0.142 at location [36.82, 233.44, 982.13, 826.39]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/83.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/83.jpg with confidence 0.436 at location [297.41, 116.67, 1806.35, 1003.06]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/30.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/30.jpg with confidence 0.356 at location [46.5, 278.39, 1002.0, 860.31]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/180.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/180.jpg with confidence 0.493 at location [121.58, 140.79, 2389.28, 1500.09]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/64.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/175.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/175.jpg with confidence 0.2 at location [1.11, 7.11, 989.59, 621.2]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/175.jpg with confidence 0.11 at location [395.66, 779.22, 568.99, 998.96]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/54.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/54.jpg with confidence 0.238 at location [629.72, 210.68, 1038.89, 512.78]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/103.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/117.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/117.jpg with confidence 0.378 at location [85.53, 413.01, 1517.02, 1372.75]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/136.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/136.jpg with confidence 0.623 at location [32.86, 223.03, 947.48, 806.92]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/43.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/43.jpg with confidence 0.409 at location [127.87, 408.65, 1480.81, 1241.77]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/75.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/75.jpg with confidence 0.258 at location [17.02, 10.59, 1509.16, 895.44]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/10.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/10.jpg with confidence 0.517 at location [25.44, 200.68, 976.99, 813.84]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/183.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/183.jpg with confidence 0.355 at location [478.67, 298.01, 2063.03, 1225.09]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/118.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/118.jpg with confidence 0.258 at location [17.02, 10.59, 1509.16, 895.44]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/185.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/185.jpg with confidence 0.262 at location [485.79, 296.56, 2059.97, 1215.47]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/185.jpg with confidence 0.106 at location [1097.2, 1181.33, 1455.07, 1279.13]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/141.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/141.jpg with confidence 0.409 at location [256.62, 86.77, 1544.77, 873.97]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/153.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/153.jpg with confidence 0.666 at location [100.78, 386.78, 1591.8, 1296.79]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/21.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/2.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/188.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/188.jpg with confidence 0.512 at location [54.11, 225.53, 934.67, 780.64]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/101.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/101.jpg with confidence 0.484 at location [26.51, 38.39, 1142.57, 700.7]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/101.jpg with confidence 0.115 at location [450.36, 661.27, 717.66, 728.22]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/101.jpg with confidence 0.151 at location [481.55, 760.41, 680.13, 1127.94]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/146.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/146.jpg with confidence 0.745 at location [48.79, 106.12, 456.31, 376.16]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/18.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/35.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/35.jpg with confidence 0.707 at location [116.36, 283.19, 1416.58, 1361.44]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/35.jpg with confidence 0.108 at location [123.26, 280.09, 1483.01, 1353.56]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/38.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/38.jpg with confidence 0.362 at location [5.55, 164.72, 994.73, 764.57]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/144.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/94.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/36.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/36.jpg with confidence 0.309 at location [35.42, 379.89, 1594.17, 1335.12]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/108.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/161.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/85.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/160.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/160.jpg with confidence 0.153 at location [101.07, 217.56, 2356.59, 1508.04]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/135.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/135.jpg with confidence 0.22 at location [59.4, 277.11, 1484.36, 1375.83]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/16.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/16.jpg with confidence 0.209 at location [13.36, 226.04, 979.46, 797.66]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/162.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/189.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/189.jpg with confidence 0.133 at location [1.32, -2.27, 2556.96, 1577.13]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/165.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/165.jpg with confidence 0.497 at location [42.76, 168.83, 758.64, 622.75]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/154.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/154.jpg with confidence 0.512 at location [55.18, 65.91, 451.87, 298.88]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/126.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/126.jpg with confidence 0.462 at location [5.57, 104.7, 995.68, 733.64]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/37.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/122.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/122.jpg with confidence 0.162 at location [28.2, 158.22, 968.96, 822.51]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/145.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/145.jpg with confidence 0.571 at location [10.63, 316.9, 2003.36, 1679.75]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/176.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/176.jpg with confidence 0.313 at location [3.21, 133.89, 981.79, 700.98]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/176.jpg with confidence 0.113 at location [31.21, 731.98, 154.34, 896.65]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/121.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/148.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/148.jpg with confidence 0.487 at location [18.19, 216.46, 790.47, 740.34]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/52.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/52.jpg with confidence 0.503 at location [64.08, 120.06, 960.79, 882.46]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/166.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/166.jpg with confidence 0.108 at location [578.45, 96.89, 1985.42, 929.82]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/125.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/125.jpg with confidence 0.324 at location [13.32, 331.42, 1571.82, 1349.36]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/174.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/174.jpg with confidence 0.101 at location [1.77, -29.88, 2547.67, 1481.27]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/197.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/197.jpg with confidence 0.18 at location [107.36, 124.48, 2475.23, 1506.52]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/3.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/3.jpg with confidence 0.405 at location [31.04, 82.34, 323.36, 260.4]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/187.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/187.jpg with confidence 0.214 at location [36.35, 37.85, 968.01, 582.09]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/59.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/59.jpg with confidence 0.648 at location [70.53, 268.79, 1162.62, 969.8]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/179.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/179.jpg with confidence 0.357 at location [28.08, 40.34, 959.39, 591.82]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/109.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/109.jpg with confidence 0.338 at location [123.69, 115.55, 1093.3, 660.16]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/26.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/26.jpg with confidence 0.266 at location [9.4, 64.38, 917.6, 630.48]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/26.jpg with confidence 0.114 at location [26.25, 39.48, 919.41, 854.31]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/182.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/137.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/137.jpg with confidence 0.365 at location [106.44, 74.93, 877.71, 905.11]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/98.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/49.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/49.jpg with confidence 0.19 at location [41.41, 327.4, 1455.96, 1220.82]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/128.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/128.jpg with confidence 0.511 at location [123.98, 363.12, 1461.04, 1296.61]\u001b[0m\n",
      "\u001b[34mDetected a photo of a dog in /opt/ml/processing/data/128.jpg with confidence 0.109 at location [1016.64, 707.02, 1447.28, 1185.62]\u001b[0m\n",
      "\u001b[34mDetected a photo of a dog in /opt/ml/processing/data/128.jpg with confidence 0.131 at location [180.96, 801.68, 410.57, 1177.71]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/193.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/193.jpg with confidence 0.621 at location [12.62, 354.69, 488.59, 638.06]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/193.jpg with confidence 0.112 at location [21.17, 362.05, 480.11, 626.78]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/1.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/1.jpg with confidence 0.589 at location [2.87, 76.06, 499.37, 413.3]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/173.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/173.jpg with confidence 0.198 at location [77.34, 102.57, 2331.01, 1434.75]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/173.jpg with confidence 0.31 at location [1009.61, 1514.11, 1509.48, 2162.64]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/13.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/13.jpg with confidence 0.423 at location [69.67, 62.53, 946.0, 857.17]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/86.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/86.jpg with confidence 0.504 at location [109.36, 235.46, 969.62, 794.25]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/105.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/105.jpg with confidence 0.395 at location [424.72, -0.02, 1473.83, 603.42]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/195.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/195.jpg with confidence 0.471 at location [28.19, 8.09, 985.64, 567.61]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/195.jpg with confidence 0.325 at location [3.11, 579.06, 252.14, 890.2]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/92.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/131.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/131.jpg with confidence 0.404 at location [70.43, 121.1, 917.44, 872.43]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/178.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/133.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/40.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/40.jpg with confidence 0.455 at location [41.37, 235.08, 1536.21, 1434.55]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/60.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/60.jpg with confidence 0.429 at location [45.33, 43.29, 963.79, 615.49]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/177.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/177.jpg with confidence 0.391 at location [93.69, 120.1, 2473.29, 1486.83]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/150.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/150.jpg with confidence 0.474 at location [271.24, 479.85, 1759.71, 1434.07]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/44.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/44.jpg with confidence 0.487 at location [34.29, 48.3, 1048.72, 663.38]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/44.jpg with confidence 0.131 at location [462.71, 693.63, 642.68, 1046.74]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/12.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a dog in /opt/ml/processing/data/12.jpg with confidence 0.208 at location [158.87, 147.68, 285.43, 267.75]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/12.jpg with confidence 0.521 at location [5.83, 16.25, 468.78, 496.23]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/72.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/181.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/181.jpg with confidence 0.301 at location [118.08, 135.76, 2330.99, 1412.29]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/24.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/24.jpg with confidence 0.324 at location [13.32, 331.42, 1571.82, 1349.36]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/82.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/82.jpg with confidence 0.258 at location [17.02, 10.59, 1509.16, 895.44]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/156.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/156.jpg with confidence 0.643 at location [27.57, 45.05, 471.74, 326.46]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/39.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/143.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/143.jpg with confidence 0.474 at location [93.96, 330.91, 1919.39, 1457.04]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/127.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/127.jpg with confidence 0.51 at location [58.24, 363.17, 1582.79, 1252.01]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/73.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/73.jpg with confidence 0.258 at location [17.02, 10.59, 1509.16, 895.44]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/147.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/147.jpg with confidence 0.357 at location [28.08, 40.34, 959.39, 591.82]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/68.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/104.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/87.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/163.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/77.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/77.jpg with confidence 0.258 at location [17.02, 10.59, 1509.16, 895.44]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/48.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/20.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/8.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/8.jpg with confidence 0.184 at location [129.85, 108.44, 1382.91, 830.29]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/66.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/170.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/190.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/190.jpg with confidence 0.234 at location [27.36, 77.6, 971.64, 636.3]\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/190.jpg with confidence 0.102 at location [432.53, 749.15, 569.08, 924.91]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/111.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/111.jpg with confidence 0.236 at location [23.71, 230.65, 983.65, 803.16]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/76.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/76.jpg with confidence 0.258 at location [17.02, 10.59, 1509.16, 895.44]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/25.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/25.jpg with confidence 0.528 at location [20.64, 329.81, 1574.15, 1347.15]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/27.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/27.jpg with confidence 0.138 at location [61.43, 253.68, 952.98, 754.55]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/171.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/171.jpg with confidence 0.535 at location [16.99, 356.57, 481.2, 638.64]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/45.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/45.jpg with confidence 0.196 at location [7.88, 346.95, 1584.16, 1326.18]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/140.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/140.jpg with confidence 0.473 at location [221.29, 42.68, 1103.06, 592.44]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/132.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/132.jpg with confidence 0.403 at location [42.61, 131.83, 460.11, 395.16]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/168.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/168.jpg with confidence 0.377 at location [0.67, 337.24, 506.2, 647.93]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/169.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/169.jpg with confidence 0.505 at location [22.25, 354.42, 479.1, 631.62]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/47.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/95.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/97.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/6.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/6.jpg with confidence 0.392 at location [46.49, 80.79, 1547.59, 1439.22]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/139.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/139.jpg with confidence 0.124 at location [100.25, 84.51, 2465.71, 1467.32]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/56.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/56.jpg with confidence 0.357 at location [148.55, 61.15, 908.37, 514.79]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/124.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/124.jpg with confidence 0.636 at location [38.32, 344.64, 1524.75, 1289.66]\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/69.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/17.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/70.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/81.jpg\u001b[0m\n",
      "\u001b[34mimage_path : /opt/ml/processing/data/99.jpg\u001b[0m\n",
      "\u001b[34mDetected a photo of a tv in /opt/ml/processing/data/99.jpg with confidence 0.595 at location [5.08, -2.26, 1479.95, 980.23]\u001b[0m\n",
      "\u001b[34mnum of results : 165\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_processor.sagemaker_session.logs_for_processing_job(job_name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cd2e075-a6e6-4bac-bc1f-2034a557cdcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-322537213286/240929-owl-vit/output/1001-04351727757322-test_result-ip-10-0-70-1.csv to output/1001-04351727757322-test_result-ip-10-0-70-1.csv\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output && mkdir ./output\n",
    "!aws s3 cp $output_path ./output --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae4bed69-9985-4309-b2b7-afc8c29cb95d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read: 1001-04351727757322-test_result-ip-10-0-70-1.csv\n",
      "\n",
      "Combined DataFrame:\n",
      "                          image_name       label_name  confidence  \\\n",
      "0     /opt/ml/processing/data/11.jpg  a photo of a tv       0.574   \n",
      "1    /opt/ml/processing/data/129.jpg  a photo of a tv       0.615   \n",
      "2    /opt/ml/processing/data/158.jpg  a photo of a tv       0.194   \n",
      "3    /opt/ml/processing/data/158.jpg  a photo of a tv       0.130   \n",
      "4    /opt/ml/processing/data/116.jpg  a photo of a tv       0.159   \n",
      "..                               ...              ...         ...   \n",
      "160    /opt/ml/processing/data/6.jpg  a photo of a tv       0.392   \n",
      "161  /opt/ml/processing/data/139.jpg  a photo of a tv       0.124   \n",
      "162   /opt/ml/processing/data/56.jpg  a photo of a tv       0.357   \n",
      "163  /opt/ml/processing/data/124.jpg  a photo of a tv       0.636   \n",
      "164   /opt/ml/processing/data/99.jpg  a photo of a tv       0.595   \n",
      "\n",
      "                                 location  \n",
      "0          [5.36, 102.36, 487.99, 385.75]  \n",
      "1        [2.47, 186.15, 1242.83, 1064.09]  \n",
      "2    [1035.31, 1513.63, 1406.04, 2240.57]  \n",
      "3       [-32.83, -22.85, 2375.5, 2479.28]  \n",
      "4           [0.74, 81.23, 299.47, 222.11]  \n",
      "..                                    ...  \n",
      "160      [46.49, 80.79, 1547.59, 1439.22]  \n",
      "161     [100.25, 84.51, 2465.71, 1467.32]  \n",
      "162       [148.55, 61.15, 908.37, 514.79]  \n",
      "163     [38.32, 344.64, 1524.75, 1289.66]  \n",
      "164        [5.08, -2.26, 1479.95, 980.23]  \n",
      "\n",
      "[165 rows x 4 columns]\n",
      "\n",
      "총 행 수: 165\n",
      "컬럼: ['image_name', 'label_name', 'confidence', 'location']\n",
      "\n",
      "처음 5행:\n",
      "                        image_name       label_name  confidence  \\\n",
      "0   /opt/ml/processing/data/11.jpg  a photo of a tv       0.574   \n",
      "1  /opt/ml/processing/data/129.jpg  a photo of a tv       0.615   \n",
      "2  /opt/ml/processing/data/158.jpg  a photo of a tv       0.194   \n",
      "3  /opt/ml/processing/data/158.jpg  a photo of a tv       0.130   \n",
      "4  /opt/ml/processing/data/116.jpg  a photo of a tv       0.159   \n",
      "\n",
      "                               location  \n",
      "0        [5.36, 102.36, 487.99, 385.75]  \n",
      "1      [2.47, 186.15, 1242.83, 1064.09]  \n",
      "2  [1035.31, 1513.63, 1406.04, 2240.57]  \n",
      "3     [-32.83, -22.85, 2375.5, 2479.28]  \n",
      "4         [0.74, 81.23, 299.47, 222.11]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dfs = []\n",
    "folder_path = \"./output\"\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            dfs.append(df)\n",
    "            print(f\"Successfully read: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {str(e)}\")\n",
    "\n",
    "if dfs:\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"\\nCombined DataFrame:\")\n",
    "    print(combined_df)\n",
    "    \n",
    "    # 추가 정보 출력\n",
    "    print(f\"\\n총 행 수: {len(combined_df)}\")\n",
    "    print(f\"컬럼: {combined_df.columns.tolist()}\")\n",
    "    \n",
    "    # 처음 5행 보기\n",
    "    print(\"\\n처음 5행:\")\n",
    "    print(combined_df.head())\n",
    "else:\n",
    "    print(\"No CSV files found in the output folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e432b8c-2c55-4351-80b4-a6bf320a5f65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
